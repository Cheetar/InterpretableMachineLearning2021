{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVoAL1q6YTEK"
   },
   "source": [
    "# Wyjaśnialne uczenia maszynowe HW1\n",
    "\n",
    "### Mateusz Sieniawski\n",
    "\n",
    "W tym notebooku zaprezentowane jest rozwiązanie pomysłu \"Impostor Forest\". Tutaj przez impostorów będę rozumiał outliery. Impostor Forest jest lasem losowym. Na samym początku wybieramy jedną klasę, której nie będziemy używali w treningu. Na przykład, powiedzmy, że w naszych danych jest 9 klas (labeli). Wybieramy jedną klasę i zakładamy, że to są dane, których model wcześniej nie widział i ma je zaklasyfikować jako outliery podczas fazy testowania modelu. Następnie uczymy każde drzewo na trochę innym zbiorze danych. Z pozostałych 8 ośmiu klas wybieramy 2, którym zamieniamy klasę na \"outlier\" tzn. drzewo dostaje poprawnie zalabelowane 6 klas oraz dwie klasy przemianowane na outliery. Każde drzewo dostaje inny losowy zbiór przemianowanych klas.\n",
    "\n",
    "Są dwie metryki do opisu jakości działania Impostor Foresta. Po pierwsze standardowe accuracy, a po drugie impostor detection rate - czyli jaki procent outlierów jest w stanie wykryć.\n",
    "\n",
    "W tym notebooku uruchomiony zostaje Impostor Forest na datasecie https://archive.ics.uci.edu/ml/datasets/Avila , a także można go łatwo odpalić dla dwóch innych datasetów.\n",
    "\n",
    "Impostor Forest osiąga gorsze accuracy niż zwykły Random Forest, ale potrafi wykrywać outliery (tzn. model może implice zwrócić predykcję \"uważam że ten obiekt jest outlierem\"). Dobierając odpowiednią wartość hiperparametrów można dobrać złoty środek pomiędzy accuracy a impostor detection rate.\n",
    "\n",
    "Ten pomysł wymyśliłem sam i z moją najlepszą wiedzą nie został on nigdzie wczeniej opublikowany."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najpewniej czuję się w Pythonie i najchętniej pracowałbym także w tym języku nad dużym projektem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VC2wv6U40Rb0",
    "outputId": "60bd8c73-7141-4466-8ec2-2c6c57e2ca4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impostor classes: 0\n",
      "Trees: 1\n",
      "Mean accuracy: 53.948%\n",
      "Mean impostor detection accuracy: 0.00\n",
      "\n",
      "Impostor classes: 0\n",
      "Trees: 5\n",
      "Mean accuracy: 51.337%\n",
      "Mean impostor detection accuracy: 0.00\n",
      "\n",
      "Impostor classes: 0\n",
      "Trees: 10\n",
      "Mean accuracy: 45.584%\n",
      "Mean impostor detection accuracy: 0.00\n",
      "\n",
      "Impostor classes: 1\n",
      "Trees: 1\n",
      "Mean accuracy: 58.855%\n",
      "Mean impostor detection accuracy: 0.34\n",
      "\n",
      "Impostor classes: 1\n",
      "Trees: 5\n",
      "Mean accuracy: 60.499%\n",
      "Mean impostor detection accuracy: 0.13\n",
      "\n",
      "Impostor classes: 1\n",
      "Trees: 10\n",
      "Mean accuracy: 62.298%\n",
      "Mean impostor detection accuracy: 0.79\n",
      "\n",
      "Impostor classes: 2\n",
      "Trees: 1\n",
      "Mean accuracy: 63.282%\n",
      "Mean impostor detection accuracy: 5.68\n",
      "\n",
      "Impostor classes: 2\n",
      "Trees: 5\n",
      "Mean accuracy: 64.985%\n",
      "Mean impostor detection accuracy: 7.28\n",
      "\n",
      "Impostor classes: 2\n",
      "Trees: 10\n",
      "Mean accuracy: 62.876%\n",
      "Mean impostor detection accuracy: 5.25\n",
      "\n",
      "Impostor classes: 3\n",
      "Trees: 1\n",
      "Mean accuracy: 55.771%\n",
      "Mean impostor detection accuracy: 41.54\n",
      "\n",
      "Impostor classes: 3\n",
      "Trees: 5\n",
      "Mean accuracy: 54.117%\n",
      "Mean impostor detection accuracy: 41.27\n",
      "\n",
      "Impostor classes: 3\n",
      "Trees: 10\n",
      "Mean accuracy: 54.942%\n",
      "Mean impostor detection accuracy: 41.37\n",
      "\n",
      "Impostor classes: 4\n",
      "Trees: 1\n",
      "Mean accuracy: 36.305%\n",
      "Mean impostor detection accuracy: 84.79\n",
      "\n",
      "Impostor classes: 4\n",
      "Trees: 5\n",
      "Mean accuracy: 37.531%\n",
      "Mean impostor detection accuracy: 86.18\n",
      "\n",
      "Impostor classes: 4\n",
      "Trees: 10\n",
      "Mean accuracy: 37.000%\n",
      "Mean impostor detection accuracy: 84.92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The implementation is based on https://machinelearningmastery.com/implement-random-forest-scratch-python/\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "import copy\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_dataset_classes(dataset):\n",
    "  classes = set()\n",
    "  for row in dataset:\n",
    "    label = row[-1]\n",
    "    classes |= set([label])\n",
    "  return classes\n",
    "\n",
    "\n",
    "def split_dataset_by_class(dataset, classes_names):\n",
    "  classes = dict()\n",
    "  for class_name in classes_names:\n",
    "    classes[class_name] = list()\n",
    "  for row in dataset:\n",
    "    label = row[-1]\n",
    "    classes[label].append(row)\n",
    "  return classes\n",
    "\n",
    "\n",
    "def evaluate_impostor_entry_rate(dataset, algorithm, no_impostor_classes, *args):\n",
    "  dataset = copy.deepcopy(dataset)\n",
    "  classes = get_dataset_classes(dataset)\n",
    "  no_dataset_classes = len(classes)\n",
    "  impostor_label = int(max(classes)) + 1\n",
    "  dataset_by_classes = split_dataset_by_class(dataset, classes)\n",
    "\n",
    "  scores = []\n",
    "  impostor_entry_accuracies = []\n",
    "  for test_impostor_class in classes:\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    trees = list()\n",
    "    test_impostor_data = dataset_by_classes[test_impostor_class]\n",
    "    for row in test_impostor_data:\n",
    "      dataset.remove(row)\n",
    "    train, validation = train_test_split(dataset, test_size=0.2, random_state=42, shuffle=True)\n",
    "    dataset_by_classes = split_dataset_by_class(train, classes)\n",
    "\n",
    "    # Split for training data and impostor test data\n",
    "    impostor_classes_combinations = itertools.combinations(list(classes - set([test_impostor_class])), no_impostor_classes)\n",
    "    for impostor_classes in set(impostor_classes_combinations):\n",
    "      not_impostors = flatten([dataset_by_classes[c] for c in classes if c not in impostor_classes and c != test_impostor_class])\n",
    "      impostors = flatten([dataset_by_classes[c] for c in classes if c in impostor_classes])\n",
    "      impostors_copy = copy.deepcopy(impostors)\n",
    "\n",
    "      for i in range(len(impostors_copy)):\n",
    "        impostors_copy[i][-1] = impostor_label\n",
    "      data = not_impostors + impostors_copy\n",
    "\n",
    "      test_set = list()\n",
    "      for row in test_impostor_data:\n",
    "          row_copy = list(row)\n",
    "          test_set.append(row_copy)\n",
    "          row_copy[-1] = None\n",
    "\n",
    "      # Build tree\n",
    "      tree = build_tree(data, max_depth, min_size, n_features)\n",
    "      trees.append(tree)\n",
    "\n",
    "    # Calculate imposter detection rate\n",
    "    predicted = [bagging_predict(trees, row) for row in test_set]\n",
    "    actual = [impostor_label for _ in range(len(test_impostor_data))]\n",
    "    impostor_entry_accuracies.append(accuracy_metric(actual, predicted))\n",
    "\n",
    "    # Calculate the standard accuracy\n",
    "    validation_set = list()\n",
    "    for row in validation:\n",
    "        row_copy = list(row)\n",
    "        validation_set.append(row_copy)\n",
    "        row_copy[-1] = None\n",
    "    predicted = [bagging_predict(trees, row) for row in validation_set]\n",
    "    actual = [row[-1] for row in validation]\n",
    "    scores.append(accuracy_metric(actual, predicted))\n",
    "\n",
    "  return impostor_entry_accuracies, scores\n",
    "\n",
    "\n",
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = list(), list()\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "    # count all samples at split point\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    # sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        # avoid divide by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        # score the group based on the score for each class\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p * p\n",
    "        # weight the group score by its relative size\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "    return gini\n",
    "\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset, n_features):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    features = list()\n",
    "    while len(features) < n_features:\n",
    "        index = randrange(len(dataset[0])-1)\n",
    "        if index not in features:\n",
    "            features.append(index)\n",
    "    for index in features:\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "\n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left, n_features)\n",
    "        split(node['left'], max_depth, min_size, n_features, depth+1)\n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right, n_features)\n",
    "        split(node['right'], max_depth, min_size, n_features, depth+1)\n",
    "\n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "    root = get_split(train, n_features)\n",
    "    split(root, max_depth, min_size, n_features, 1)\n",
    "    return root\n",
    "\n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(dataset, ratio):\n",
    "    sample = list()\n",
    "    n_sample = round(len(dataset) * ratio)\n",
    "    while len(sample) < n_sample:\n",
    "        index = randrange(len(dataset))\n",
    "        sample.append(dataset[index])\n",
    "    return sample\n",
    "\n",
    "# Make a prediction with a list of bagged trees\n",
    "def bagging_predict(trees, row):\n",
    "    predictions = [predict(tree, row) for tree in trees]\n",
    "    return max(set(predictions), key=predictions.count)\n",
    "\n",
    "# Random Forest Algorithm\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "    trees = list()\n",
    "    for i in range(n_trees):\n",
    "        sample = subsample(train, sample_size)\n",
    "        tree = build_tree(sample, max_depth, min_size, n_features)\n",
    "        trees.append(tree)\n",
    "    predictions = [bagging_predict(trees, row) for row in test]\n",
    "    return(predictions)\n",
    "\n",
    "def preprocess_ecoli(dataset):\n",
    "  dataset = dataset[1:]\n",
    "  label_map = {'cp': 0,\n",
    "               'im': 1,\n",
    "               'pp': 2,\n",
    "               'imU': 3,\n",
    "               'om': 4,\n",
    "               'omL': 5,\n",
    "               'imL': 6,\n",
    "               'imS': 7}\n",
    "  for i in range(len(dataset)):\n",
    "    dataset[i][-1] = label_map[dataset[i][-1]]\n",
    "  return dataset\n",
    "\n",
    "def preprocess_avila(dataset):\n",
    "  label_map = {'A': 0,\n",
    "               'B': 1,\n",
    "               'C': 2,\n",
    "               'D': 3,\n",
    "               'E': 4,\n",
    "               'F': 5,\n",
    "               'G': 6,\n",
    "               'H': 7,\n",
    "               'I': 8,\n",
    "               'W': 9,\n",
    "               'X': 10,\n",
    "               'Y': 11}\n",
    "  for i in range(len(dataset)):\n",
    "    dataset[i][-1] = label_map[dataset[i][-1]]\n",
    "  return dataset\n",
    "\n",
    "# Test the random forest algorithm\n",
    "seed(2)\n",
    "# load and prepare data\n",
    "# https://archive.ics.uci.edu/ml/machine-learning-databases/glass/\n",
    "#filename = '/content/glass.data'\n",
    "# https://www.openml.org/d/39\n",
    "#filename = '/content/dataset_ecoli.csv'\n",
    "# https://archive.ics.uci.edu/ml/datasets/Avila\n",
    "filename = 'content/avila-tr.txt'\n",
    "dataset = load_csv(filename)\n",
    "#dataset = preprocess_ecoli(dataset)\n",
    "dataset = preprocess_avila(dataset)\n",
    "#random.shuffle(dataset)\n",
    "dataset = dataset[:500]\n",
    "# convert string attributes to integers\n",
    "for i in range(0, len(dataset[0])-1):\n",
    "  str_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "n_folds = 5\n",
    "max_depth = 10\n",
    "min_size = 1\n",
    "sample_size = 1.0\n",
    "n_features = int(sqrt(len(dataset[0])-1))\n",
    "\n",
    "for n_impostor_classes in range(5):\n",
    "  for n_trees in [1, 5, 10]:\n",
    "\n",
    "    impostor_entry_accuracies, scores = evaluate_impostor_entry_rate(dataset, random_forest, n_impostor_classes, max_depth, min_size, sample_size, n_trees, n_features)\n",
    "    print('Impostor classes: %d' % n_impostor_classes)\n",
    "    print('Trees: %d' % n_trees)\n",
    "    print('Mean accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "    print('Mean impostor detection accuracy: %.2f' % (sum(impostor_entry_accuracies)/float(len(impostor_entry_accuracies))))\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CQYPumQ-wbk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of ImpostorForest",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
